{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfUpO1rPN6lo"
   },
   "source": [
    "# 1- Mise en place de l'environnement de travail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\tInstaller Spark \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Java8\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# download spark3.0.0\n",
    "!wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
    "# unzip it\n",
    "!tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
    "# install findspark \n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandavro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Définir la valeur de la variable d’environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.12:3.0.1 pyspark-shell'\n",
    "#os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[4] pyspark-shell\"\n",
    "\n",
    "import findspark\n",
    "findspark.init(\"spark-3.0.1-bin-hadoop2.7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\tCréer un objet SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LjnjzBsAAMWG"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "conf = SparkConf().setAppName(\"Projet Big Data\").set(\"spark.driver.memory\", \"10g\") \n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jj32aIsAQLA",
    "outputId": "3669708c-23e8-40ae-8955-887658667477"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-VNSDDUS:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Projet Big Data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Projet Big Data>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Créer un objet de type SparkSession. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "z_VA49YZwL2C"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Gc9YFiqOBOf"
   },
   "source": [
    "# 2- Données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Charger les données dans deux variables de type RDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "44ESiroDZCOv"
   },
   "outputs": [],
   "source": [
    "altheism = sc.wholeTextFiles('./data/20_newsgroups/alt.atheism/*')\n",
    "baseball = sc.wholeTextFiles('./data/20_newsgroups/rec.sport.baseball/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4jbU22aLtpsK"
   },
   "outputs": [],
   "source": [
    "altheismPath = altheism.map(lambda a : a[0])\n",
    "altheismText = altheism.map(lambda a : a[1])\n",
    "\n",
    "baseballPath = baseball.map(lambda a : a[0])\n",
    "baseballText = baseball.map(lambda a : a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HlOotWbaa4D8",
    "outputId": "556c8dd3-bc51-4dcd-bae9-0e331a789923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(altheism.count())\n",
    "print(baseball.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\tSéparer le corps du message de l’entête."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HV4j7GWsMMQO"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_all(a_str, sub):\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = a_str.find(sub, start)\n",
    "        if start == -1: return\n",
    "        yield start\n",
    "        start += len(sub) # use start += 1 to find overlapping matches\n",
    "\n",
    "def findSeparator(elem):\n",
    "    findSeparator = re.search('Lines: [0-9]+\\n+',elem)\n",
    "    if (findSeparator):\n",
    "        return findSeparator.end()\n",
    "    else:\n",
    "        findSeparator = re.search('GMT\\n', elem) # Dans quelques documents de baseball il n'y a pas de \"Lines: \",  dans ce cas là la délimitation est en fonction de la date\n",
    "    return findSeparator.end()\n",
    "\n",
    "def extractHeadAndbody(elem):\n",
    "    separator = findSeparator(elem)\n",
    "    return (elem[0:separator].strip(), elem[separator:-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "W3SykNtOVDl0"
   },
   "outputs": [],
   "source": [
    "altheismSeparate = altheismText.map(extractHeadAndbody)\n",
    "baseballSeparate = baseballText.map(extractHeadAndbody)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)\tExtraire quelques champs de l’entête, par exemple l’organisation et la catégorie (champ “News-groups”).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MpJkKWQZb3-I"
   },
   "outputs": [],
   "source": [
    "def extractSomeFieldsHead(elem, category):\n",
    "    head = elem[0]\n",
    "    dictInformations = dict()\n",
    "\n",
    "    newGroup = 'Newsgroups: '\n",
    "    findNewGroup = head.find(newGroup)\n",
    "    endNewGroup = head[findNewGroup:-1].find('\\n')\n",
    "    dictInformations['newsGroups'] = head[(findNewGroup + len(newGroup)):(findNewGroup + endNewGroup)].strip()\n",
    "\n",
    "    lines = 'Lines: '\n",
    "    findLines = head.find(lines)\n",
    "    dictInformations['lines'] = head[(findLines + len(lines)):-1].strip()\n",
    "\n",
    "    organization = 'Organization: '\n",
    "    findOrganization = head.find(organization)\n",
    "    endOrganization = head[findOrganization:-1].find('\\n')\n",
    "    dictInformations['organization'] = head[(findOrganization + len(organization)):(findOrganization + endOrganization)].strip()\n",
    "\n",
    "    subject = 'Subject: '\n",
    "    findSubject = head.find(subject)\n",
    "    endSubject = head[findSubject:-1].find('\\n')\n",
    "    dictInformations['subject'] = head[(findSubject + len(subject)):(findSubject + endSubject)].strip()\n",
    "\n",
    "    date = 'Date: '\n",
    "    findDate = head.find(date)\n",
    "    endDate = head[findDate:-1].find('\\n')\n",
    "    dictInformations['date'] = head[(findDate + len(date)):(findDate + endDate)].strip()\n",
    "\n",
    "    dictInformations['category'] = category\n",
    "\n",
    "    return (dictInformations, elem[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uBK9UtCrhigd"
   },
   "outputs": [],
   "source": [
    "altheismSomeHeadBody = altheismSeparate.map(lambda elem: extractSomeFieldsHead(elem,'altheism'))\n",
    "baseballSomeHeadBody = baseballSeparate.map(lambda elem: extractSomeFieldsHead(elem,'baseball'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f)\tFusionner les deux RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8ubKqKHK0d5A"
   },
   "outputs": [],
   "source": [
    "altheismBaseball = altheismSomeHeadBody.union(baseballSomeHeadBody)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g)\tTransformer le nouveau RDD obtenu pour que chaque élément soit de type pyspark.sql.Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ssXEDjdHDGPI"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def elemToRowObject(elem):\n",
    "    head = elem[0]\n",
    "    body = elem[1]\n",
    "    return Row(newsGroups = head['newsGroups'], category = head['category'], lines = head['lines'], organization=head['organization'], subject=head['subject'], date=head['date'], body=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "pVkMt54t3M8a"
   },
   "outputs": [],
   "source": [
    "rowAltheismBaseball = altheismBaseball.map(elemToRowObject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (h)\tCréer un objet de type DataFrame à partir du RDD précédent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iVMGTFmoFpEZ",
    "outputId": "f416e182-f913-4f55-c537-36e2b12fbf2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- newsGroups: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- lines: string (nullable = true)\n",
      " |-- organization: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      "\n",
      "+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|          newsGroups|category|lines|        organization|             subject|                date|                body|\n",
      "+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|alt.atheism,alt.a...|altheism|   29|Mantis Consultant...|Alt.Atheism FAQ: ...|Mon, 29 Mar 1993 ...|Archive-name: ath...|\n",
      "|alt.atheism,alt.a...|altheism|   64|Mantis Consultant...|Alt.Atheism FAQ: ...|Mon, 5 Apr 1993 1...|Archive-name: ath...|\n",
      "|         alt.atheism|altheism|    9|Technical Univers...|   Re: Gospel Dating|Mon, 5 Apr 1993 1...|In article <65974...|\n",
      "|alt.atheism,alt.p...|altheism|    2|Mantis Consultant...|Re: university vi...|Mon, 5 Apr 1993 1...|dmn@kepler.unh.ed...|\n",
      "|alt.atheism,soc.m...|altheism|    1|        IBM Research|Re: [soc.motss, e...|Mon, 05 Apr 93 18...|In article <N4HY....|\n",
      "|         alt.atheism|altheism|   11|Technical Univers...|Re: A visit from ...|Mon, 5 Apr 1993 1...|In article <1993A...|\n",
      "|         alt.atheism|altheism|    1|California Instit...|Re: Political Ath...|2 Apr 93 19:05:57...|NNTP-Posting-Host...|\n",
      "|         alt.atheism|altheism|    2|Technical Univers...|Re: An Anecdote a...|Mon, 5 Apr 1993 1...|In article <11412...|\n",
      "|         alt.atheism|altheism|    5|California Instit...|Re: <Political At...|2 Apr 1993 20:43:...|Message-ID: <1pi8...|\n",
      "|         alt.atheism|altheism|     |California Instit...|Re: >>>>>>Pompous...|2 Apr 93 20:57:33...|NNTP-Posting-Host...|\n",
      "|         alt.atheism|altheism|    1|California Instit...|Re: >>>>>>Pompous...|2 Apr 93 21:01:40...|NNTP-Posting-Host...|\n",
      "|         alt.atheism|altheism|    1|California Instit...|Re: Keith Schneid...|2 Apr 93 21:10:38...|NNTP-Posting-Host...|\n",
      "|alt.atheism,talk....|altheism|    1|California Instit...|    Re: Albert Sabin|2 Apr 93 20:54:30...|NNTP-Posting-Host...|\n",
      "|         alt.atheism|altheism|    2|California Instit...|Re: Keith Schneid...|2 Apr 93 21:07:36...|NNTP-Posting-Host...|\n",
      "|         alt.atheism|altheism|    4|California Instit...|Re: <Political At...|2 Apr 1993 21:22:...|Message-ID: <1pia...|\n",
      "|         alt.atheism|altheism|    5|California Instit...|Re: <Political At...|2 Apr 93 21:35:33...|NNTP-Posting-Host...|\n",
      "|         alt.atheism|altheism|    2|California Instit...|Re: Political Ath...|2 Apr 93 21:40:41...|NNTP-Posting-Host...|\n",
      "|         alt.atheism|altheism|    2|Tektronix, Inc., ...|Re: Don't more in...|5 Apr 93 19:54:50...|In article <29428...|\n",
      "|         alt.atheism|altheism|    3|Tektronix, Inc., ...|Re: Ancient islam...|5 Apr 93 20:02:06...|In article <1993A...|\n",
      "|         alt.atheism|altheism|    4|Tektronix, Inc., ...|Re: <Political At...|5 Apr 93 20:16:20...|In article <1993A...|\n",
      "+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(rowAltheismBaseball)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i,j)\tSauvegarder la DataFrame au format Avro et Parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"avro\").save(\"./output/df.avro\")\n",
    "df.write.parquet(\"./output/df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-nxmFRTOEzP"
   },
   "source": [
    "# 3- Analyse descriptive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\tVérifier qu’on a bien deux catégories différentes de documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYWkebMeIT7B",
    "outputId": "34b022af-caef-4c13-d9d2-32bb22fadda8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|category|\n",
      "+--------+\n",
      "|altheism|\n",
      "|baseball|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('category').distinct().show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Seul deux catégories sont présentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\tDonner le nombre d’organisations différentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DeX_2XMoHbn_",
    "outputId": "8d037e78-c05e-4ca6-a29c-c53a647186ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "578"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('organization').distinct().count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En tout 578 organisations différentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Suivant les champs extraits, donner d’autres statistiques descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2xQtJ79Ke2_",
    "outputId": "2579f027-23b1-477a-d647-75f63731584d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.distinct().count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nombre de documents : 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4nqfIWiJ4iy",
    "outputId": "eaf133ba-a225-4ad7-9322-2db3cd72dcfa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "762"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('subject').distinct().count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Le dataset contient 762 sujets différents (sur un total de 2000 documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFQQcxQcLwJQ"
   },
   "source": [
    "# 4- Transformation du texte et clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\tDécouper les documents en listes de mots à l’aide de Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Fc852NLiL1PP"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ZMx2yMmvbYjs"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "pF58gV6xiO_f"
   },
   "outputs": [],
   "source": [
    "listWords = wordsData.select('words').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KT-rWKqKb6ST",
    "outputId": "f578db55-ce2d-4bc1-891b-71391ce701ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          newsGroups|category|lines|        organization|             subject|                date|                body|               words|\n",
      "+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|alt.atheism,alt.a...|altheism|   29|Mantis Consultant...|Alt.Atheism FAQ: ...|Mon, 29 Mar 1993 ...|Archive-name: ath...|[archive-name:, a...|\n",
      "|alt.atheism,alt.a...|altheism|   64|Mantis Consultant...|Alt.Atheism FAQ: ...|Mon, 5 Apr 1993 1...|Archive-name: ath...|[archive-name:, a...|\n",
      "|         alt.atheism|altheism|    9|Technical Univers...|   Re: Gospel Dating|Mon, 5 Apr 1993 1...|In article <65974...|[in, article, <65...|\n",
      "|alt.atheism,alt.p...|altheism|    2|Mantis Consultant...|Re: university vi...|Mon, 5 Apr 1993 1...|dmn@kepler.unh.ed...|[dmn@kepler.unh.e...|\n",
      "|alt.atheism,soc.m...|altheism|    1|        IBM Research|Re: [soc.motss, e...|Mon, 05 Apr 93 18...|In article <N4HY....|[in, article, <n4...|\n",
      "|         alt.atheism|altheism|   11|Technical Univers...|Re: A visit from ...|Mon, 5 Apr 1993 1...|In article <1993A...|[in, article, <19...|\n",
      "|         alt.atheism|altheism|    1|California Instit...|Re: Political Ath...|2 Apr 93 19:05:57...|NNTP-Posting-Host...|[nntp-posting-hos...|\n",
      "|         alt.atheism|altheism|    2|Technical Univers...|Re: An Anecdote a...|Mon, 5 Apr 1993 1...|In article <11412...|[in, article, <11...|\n",
      "|         alt.atheism|altheism|    5|California Instit...|Re: <Political At...|2 Apr 1993 20:43:...|Message-ID: <1pi8...|[message-id:, <1p...|\n",
      "|         alt.atheism|altheism|     |California Instit...|Re: >>>>>>Pompous...|2 Apr 93 20:57:33...|NNTP-Posting-Host...|[nntp-posting-hos...|\n",
      "|         alt.atheism|altheism|    1|California Instit...|Re: >>>>>>Pompous...|2 Apr 93 21:01:40...|NNTP-Posting-Host...|[nntp-posting-hos...|\n",
      "|         alt.atheism|altheism|    1|California Instit...|Re: Keith Schneid...|2 Apr 93 21:10:38...|NNTP-Posting-Host...|[nntp-posting-hos...|\n",
      "|alt.atheism,talk....|altheism|    1|California Instit...|    Re: Albert Sabin|2 Apr 93 20:54:30...|NNTP-Posting-Host...|[nntp-posting-hos...|\n",
      "|         alt.atheism|altheism|    2|California Instit...|Re: Keith Schneid...|2 Apr 93 21:07:36...|NNTP-Posting-Host...|[nntp-posting-hos...|\n",
      "|         alt.atheism|altheism|    4|California Instit...|Re: <Political At...|2 Apr 1993 21:22:...|Message-ID: <1pia...|[message-id:, <1p...|\n",
      "|         alt.atheism|altheism|    5|California Instit...|Re: <Political At...|2 Apr 93 21:35:33...|NNTP-Posting-Host...|[nntp-posting-hos...|\n",
      "|         alt.atheism|altheism|    2|California Instit...|Re: Political Ath...|2 Apr 93 21:40:41...|NNTP-Posting-Host...|[nntp-posting-hos...|\n",
      "|         alt.atheism|altheism|    2|Tektronix, Inc., ...|Re: Don't more in...|5 Apr 93 19:54:50...|In article <29428...|[in, article, <29...|\n",
      "|         alt.atheism|altheism|    3|Tektronix, Inc., ...|Re: Ancient islam...|5 Apr 93 20:02:06...|In article <1993A...|[in, article, <19...|\n",
      "|         alt.atheism|altheism|    4|Tektronix, Inc., ...|Re: <Political At...|5 Apr 93 20:16:20...|In article <1993A...|[in, article, <19...|\n",
      "+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordsData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "l7RvB2nkcJt0"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filteredWords\")\n",
    "wordsFiltered = remover.transform(wordsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4boIZw3wcUA5",
    "outputId": "a91fd2b5-39fb-43b3-aa91-ba258673dc9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          newsGroups|category|lines|        organization|             subject|                date|                body|               words|       filteredWords|\n",
      "+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|alt.atheism,alt.a...|altheism|   29|Mantis Consultant...|Alt.Atheism FAQ: ...|Mon, 29 Mar 1993 ...|Archive-name: ath...|[archive-name:, a...|[archive-name:, a...|\n",
      "|alt.atheism,alt.a...|altheism|   64|Mantis Consultant...|Alt.Atheism FAQ: ...|Mon, 5 Apr 1993 1...|Archive-name: ath...|[archive-name:, a...|[archive-name:, a...|\n",
      "|         alt.atheism|altheism|    9|Technical Univers...|   Re: Gospel Dating|Mon, 5 Apr 1993 1...|In article <65974...|[in, article, <65...|[article, <65974@...|\n",
      "|alt.atheism,alt.p...|altheism|    2|Mantis Consultant...|Re: university vi...|Mon, 5 Apr 1993 1...|dmn@kepler.unh.ed...|[dmn@kepler.unh.e...|[dmn@kepler.unh.e...|\n",
      "|alt.atheism,soc.m...|altheism|    1|        IBM Research|Re: [soc.motss, e...|Mon, 05 Apr 93 18...|In article <N4HY....|[in, article, <n4...|[article, <n4hy.9...|\n",
      "|         alt.atheism|altheism|   11|Technical Univers...|Re: A visit from ...|Mon, 5 Apr 1993 1...|In article <1993A...|[in, article, <19...|[article, <1993ap...|\n",
      "|         alt.atheism|altheism|    1|California Instit...|Re: Political Ath...|2 Apr 93 19:05:57...|NNTP-Posting-Host...|[nntp-posting-hos...|[nntp-posting-hos...|\n",
      "|         alt.atheism|altheism|    2|Technical Univers...|Re: An Anecdote a...|Mon, 5 Apr 1993 1...|In article <11412...|[in, article, <11...|[article, <114127...|\n",
      "|         alt.atheism|altheism|    5|California Instit...|Re: <Political At...|2 Apr 1993 20:43:...|Message-ID: <1pi8...|[message-id:, <1p...|[message-id:, <1p...|\n",
      "|         alt.atheism|altheism|     |California Instit...|Re: >>>>>>Pompous...|2 Apr 93 20:57:33...|NNTP-Posting-Host...|[nntp-posting-hos...|[nntp-posting-hos...|\n",
      "|         alt.atheism|altheism|    1|California Instit...|Re: >>>>>>Pompous...|2 Apr 93 21:01:40...|NNTP-Posting-Host...|[nntp-posting-hos...|[nntp-posting-hos...|\n",
      "|         alt.atheism|altheism|    1|California Instit...|Re: Keith Schneid...|2 Apr 93 21:10:38...|NNTP-Posting-Host...|[nntp-posting-hos...|[nntp-posting-hos...|\n",
      "|alt.atheism,talk....|altheism|    1|California Instit...|    Re: Albert Sabin|2 Apr 93 20:54:30...|NNTP-Posting-Host...|[nntp-posting-hos...|[nntp-posting-hos...|\n",
      "|         alt.atheism|altheism|    2|California Instit...|Re: Keith Schneid...|2 Apr 93 21:07:36...|NNTP-Posting-Host...|[nntp-posting-hos...|[nntp-posting-hos...|\n",
      "|         alt.atheism|altheism|    4|California Instit...|Re: <Political At...|2 Apr 1993 21:22:...|Message-ID: <1pia...|[message-id:, <1p...|[message-id:, <1p...|\n",
      "|         alt.atheism|altheism|    5|California Instit...|Re: <Political At...|2 Apr 93 21:35:33...|NNTP-Posting-Host...|[nntp-posting-hos...|[nntp-posting-hos...|\n",
      "|         alt.atheism|altheism|    2|California Instit...|Re: Political Ath...|2 Apr 93 21:40:41...|NNTP-Posting-Host...|[nntp-posting-hos...|[nntp-posting-hos...|\n",
      "|         alt.atheism|altheism|    2|Tektronix, Inc., ...|Re: Don't more in...|5 Apr 93 19:54:50...|In article <29428...|[in, article, <29...|[article, <294288...|\n",
      "|         alt.atheism|altheism|    3|Tektronix, Inc., ...|Re: Ancient islam...|5 Apr 93 20:02:06...|In article <1993A...|[in, article, <19...|[article, <1993ap...|\n",
      "|         alt.atheism|altheism|    4|Tektronix, Inc., ...|Re: <Political At...|5 Apr 93 20:16:20...|In article <1993A...|[in, article, <19...|[article, <1993ap...|\n",
      "+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordsFiltered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\tCréer une représentation vectorielle des documents à l’aide de HashingTF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "6w2I5QvLbijW"
   },
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"filteredWords\", outputCol=\"features\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSzlgFeYbrWA",
    "outputId": "66103885-d65a-4299-fa64-860fc25bcd49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          newsGroups|category|lines|        organization|             subject|                date|                body|               words|       filteredWords|            features|\n",
      "+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|alt.atheism,alt.a...|altheism|   29|Mantis Consultant...|Alt.Atheism FAQ: ...|Mon, 29 Mar 1993 ...|Archive-name: ath...|[archive-name:, a...|[archive-name:, a...|(20,[0,1,2,3,4,5,...|\n",
      "|alt.atheism,alt.a...|altheism|   64|Mantis Consultant...|Alt.Atheism FAQ: ...|Mon, 5 Apr 1993 1...|Archive-name: ath...|[archive-name:, a...|[archive-name:, a...|(20,[0,1,2,3,4,5,...|\n",
      "|         alt.atheism|altheism|    9|Technical Univers...|   Re: Gospel Dating|Mon, 5 Apr 1993 1...|In article <65974...|[in, article, <65...|[article, <65974@...|(20,[0,1,2,3,4,5,...|\n",
      "|alt.atheism,alt.p...|altheism|    2|Mantis Consultant...|Re: university vi...|Mon, 5 Apr 1993 1...|dmn@kepler.unh.ed...|[dmn@kepler.unh.e...|[dmn@kepler.unh.e...|(20,[0,1,2,3,4,5,...|\n",
      "|alt.atheism,soc.m...|altheism|    1|        IBM Research|Re: [soc.motss, e...|Mon, 05 Apr 93 18...|In article <N4HY....|[in, article, <n4...|[article, <n4hy.9...|(20,[0,1,2,3,4,5,...|\n",
      "|         alt.atheism|altheism|   11|Technical Univers...|Re: A visit from ...|Mon, 5 Apr 1993 1...|In article <1993A...|[in, article, <19...|[article, <1993ap...|(20,[0,1,2,3,4,5,...|\n",
      "|         alt.atheism|altheism|    1|California Instit...|Re: Political Ath...|2 Apr 93 19:05:57...|NNTP-Posting-Host...|[nntp-posting-hos...|[nntp-posting-hos...|(20,[0,1,2,3,4,6,...|\n",
      "|         alt.atheism|altheism|    2|Technical Univers...|Re: An Anecdote a...|Mon, 5 Apr 1993 1...|In article <11412...|[in, article, <11...|[article, <114127...|(20,[0,1,2,3,4,5,...|\n",
      "|         alt.atheism|altheism|    5|California Instit...|Re: <Political At...|2 Apr 1993 20:43:...|Message-ID: <1pi8...|[message-id:, <1p...|[message-id:, <1p...|(20,[0,1,2,3,4,5,...|\n",
      "|         alt.atheism|altheism|     |California Instit...|Re: >>>>>>Pompous...|2 Apr 93 20:57:33...|NNTP-Posting-Host...|[nntp-posting-hos...|[nntp-posting-hos...|(20,[0,1,3,4,5,7,...|\n",
      "|         alt.atheism|altheism|    1|California Instit...|Re: >>>>>>Pompous...|2 Apr 93 21:01:40...|NNTP-Posting-Host...|[nntp-posting-hos...|[nntp-posting-hos...|(20,[0,1,2,3,4,5,...|\n",
      "|         alt.atheism|altheism|    1|California Instit...|Re: Keith Schneid...|2 Apr 93 21:10:38...|NNTP-Posting-Host...|[nntp-posting-hos...|[nntp-posting-hos...|(20,[0,3,4,5,6,7,...|\n",
      "|alt.atheism,talk....|altheism|    1|California Instit...|    Re: Albert Sabin|2 Apr 93 20:54:30...|NNTP-Posting-Host...|[nntp-posting-hos...|[nntp-posting-hos...|(20,[0,1,3,4,5,6,...|\n",
      "|         alt.atheism|altheism|    2|California Instit...|Re: Keith Schneid...|2 Apr 93 21:07:36...|NNTP-Posting-Host...|[nntp-posting-hos...|[nntp-posting-hos...|(20,[0,1,2,3,4,5,...|\n",
      "|         alt.atheism|altheism|    4|California Instit...|Re: <Political At...|2 Apr 1993 21:22:...|Message-ID: <1pia...|[message-id:, <1p...|[message-id:, <1p...|(20,[0,1,2,3,4,5,...|\n",
      "|         alt.atheism|altheism|    5|California Instit...|Re: <Political At...|2 Apr 93 21:35:33...|NNTP-Posting-Host...|[nntp-posting-hos...|[nntp-posting-hos...|(20,[0,1,2,3,4,5,...|\n",
      "|         alt.atheism|altheism|    2|California Instit...|Re: Political Ath...|2 Apr 93 21:40:41...|NNTP-Posting-Host...|[nntp-posting-hos...|[nntp-posting-hos...|(20,[0,1,3,4,5,6,...|\n",
      "|         alt.atheism|altheism|    2|Tektronix, Inc., ...|Re: Don't more in...|5 Apr 93 19:54:50...|In article <29428...|[in, article, <29...|[article, <294288...|(20,[0,1,2,3,4,5,...|\n",
      "|         alt.atheism|altheism|    3|Tektronix, Inc., ...|Re: Ancient islam...|5 Apr 93 20:02:06...|In article <1993A...|[in, article, <19...|[article, <1993ap...|(20,[0,1,2,3,4,5,...|\n",
      "|         alt.atheism|altheism|    4|Tektronix, Inc., ...|Re: <Political At...|5 Apr 93 20:16:20...|In article <1993A...|[in, article, <19...|[article, <1993ap...|(20,[0,1,2,3,4,5,...|\n",
      "+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurizedData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Utiliser l’algorithme KMeans avec un nombre du cluster égal à 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "0NiZS7XM3-o7"
   },
   "outputs": [],
   "source": [
    "trueCategory = featurizedData.select('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "hOJSo4Jo4C8z"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "featuresData = featurizedData.select('features')\n",
    "trueCategory = trueCategory.withColumn(\"category\", when(col(\"category\") == \"altheism\", 0).when(col(\"category\") == \"baseball\",1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trueCategory = trueCategory.select(F.col(\"category\").alias(\"label\"))\n",
    "dataset = featuresData.join(df_trueCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataset_df = dataset.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, X_test) = dataset.randomSplit([0.7, 0.3], seed = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "Em2ahjJjp5_0"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Analyser les résultats et la qualité de la partition obtenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "cQ6EEwpMr0-2"
   },
   "outputs": [],
   "source": [
    "predictions_kmeans = model.transform(X_test).withColumn(\"prediction\",F.col(\"prediction\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3350032426420757"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'accuracy obtenue est faible. Sachant qu'il existe 2 cluster, un classifieur naïf basé uniquement sur le hasard (sous l'hypothése i.i.d) arriverait à obtenir de meilleurs résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.257279428633879e-07"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "normalized_mutual_info_score(y_test, predictions_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La NMI est très proche de zéro, cela confirme le résulat précédent et indique un mauvais partionnement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Comparer avec ce qu’on aurait obtenu avec l’implémentation de k-means du package scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "series = pandas_dataset_df['features'].apply(lambda x : np.array(x.toArray())).to_numpy().reshape(-1,1)\n",
    "features = np.apply_along_axis(lambda x : x[0], 1, series)\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, pandas_dataset_df['label'] ,test_size=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "sklearn_kmeans = KMeans(\n",
    "    n_clusters=2, init='random',\n",
    "    n_init=10, max_iter=300, \n",
    "    tol=1e-04, random_state=0\n",
    ")\n",
    "sklearn_kmeans_model = sklearn_kmeans.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_kmeans_predictions = sklearn_kmeans_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.01      0.02    598837\n",
      "           1       0.50      0.99      0.67    601163\n",
      "\n",
      "    accuracy                           0.50   1200000\n",
      "   macro avg       0.50      0.50      0.34   1200000\n",
      "weighted avg       0.50      0.50      0.34   1200000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, sklearn_kmeans_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résulats obtenus refletent une classification similaire à celle qu'on obtiendrai en nous basant uniquement sur le hasard. Ces résultats sont cependant meilleurs que ceux résultant d'un kmeans implémenté par spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.257279428633879e-07"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_mutual_info_score(y_test, sklearn_kmeans_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La NMI est exactement la même que celle obtenue précédemment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Implémentation de K-means unidimensionnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Définir la fonction compute_centroids qui prend en argument deux RDD (points ,cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "bWyLkl-JNXgc"
   },
   "outputs": [],
   "source": [
    "def compute_centroids(points,cluster_ids):\n",
    "    #poinds et cluster_ids sont deux rdd\n",
    "    tuple_cluster_points = cluster_ids.zip(points)\n",
    "    sum_by_cluster_id = tuple_cluster_points.reduceByKey(lambda a,b: a+b)\n",
    "    count_by_cluster_id = sc.parallelize(tuple_cluster_points.countByKey().items())\n",
    "    tmp = sum_by_cluster_id.union(count_by_cluster_id)\n",
    "    result = tmp.reduceByKey(lambda a,b: a/b)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Définir la fonction assign_clusters qui prend en argument deux RDDs (points,centroids) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "iYojWhmFdFQP"
   },
   "outputs": [],
   "source": [
    "def squared_distances(point,listMean):\n",
    "    result = [0] * len(listMean)\n",
    "    for i, value in enumerate(listMean):\n",
    "        result[i] = (value - point) ** 2\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "XI0zd5fVLbV7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def assign_clusters(points,centroids):\n",
    "    clusters = centroids.map(lambda a : a[0]).collect()\n",
    "    means = centroids.map(lambda a : a[1]).collect() \n",
    "    distances = [squared_distances(point, means) for point in points.collect()]\n",
    "    return sc.parallelize([np.argmin(distance) for distance in distances])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Implémenter l’étape d’initialisation et l’itération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "jLapXpva6tpE"
   },
   "outputs": [],
   "source": [
    "def initCentroids(data, numClusters):\n",
    "    sample = sc.parallelize(data.sample(False, 0.1).limit(1))\n",
    "    centroids = sample.map(lambda point : point[1][:-1])\n",
    "    return centroids.zipWithIndex().map(lambda point : (point[1], point[0]))\n",
    "\n",
    "\n",
    "\n",
    "def kmeans(data, numClusters, nb_iter=10):\n",
    "    centroids = initCentroids(data, numClusters)\n",
    "    iterations = 0\n",
    "    while iterations != nb_iter:\n",
    "        iterations += 1\n",
    "        dataMinDistance = assign_clusters(data, centroids)\n",
    "        newCentroids = compute_centroids(dataMinDistance)\n",
    "        intraClusterDistances = squared_distances(dataMinDistance)\n",
    "\n",
    "        centroids = sc.parallelize(newCentroids.collect())\n",
    "    clusters = assign_clusters(data, centroids)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoUe7AK18371"
   },
   "source": [
    "# 6- Spherical k-means et k-means multidimensionnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Adapter l’implémentation précédente pour gérer le cas multidimensionnel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "zh0L69jv88xD"
   },
   "outputs": [],
   "source": [
    "def compute_centroids_multi(points, cluster_ids):\n",
    "    #poinds et cluster_ids sont deux rdd\n",
    "    tuple_cluster_points = cluster_ids.zip(points)\n",
    "    sum_by_cluster_id = tuple_cluster_points.reduceByKey(lambda a,b: np.array(a)+np.array(b))\n",
    "    count_by_cluster_id = sc.parallelize(tuple_cluster_points.countByKey().items())\n",
    "    tmp = sum_by_cluster_id.union(count_by_cluster_id)\n",
    "    result = tmp.reduceByKey(lambda a,b: np.array(a)  / np.array(b))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "sh6c-XEcETx3"
   },
   "outputs": [],
   "source": [
    "def squared_distances_multi(point,listMean):\n",
    "    result = [[0 for j in range(len(point))] for i in range(len(listMean))]\n",
    "    for i, mean in enumerate(listMean):\n",
    "        for j, coordPoint in enumerate(point):\n",
    "            result[i] = (mean[j] - coordPoint) ** 2\n",
    "    return result #liste des carrés des distances entre le point et les différentes moyennes des clusters, REVOIR SI C'EST ÇA ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "j5LTJpXiSzhk"
   },
   "outputs": [],
   "source": [
    "def assign_clusters_multi(points,centroids):\n",
    "    clusters = centroids.map(lambda a : a[0]).collect()\n",
    "    means = centroids.map(lambda a : a[1]).collect() #Quelle supposition a-t-on faite pour effectuer cette opération ? --> Que les valeurs sont ordonnées (cluster 0 puis cluster 1...)\n",
    "    distances = [squared_distances(point, means) for point in points.collect()]\n",
    "    return sc.parallelize([np.argmin(distance) for distance in distances])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8XV9gJ5JT4xY",
    "outputId": "926f8707-a6a4-4c41-f4a9-270783dc934f"
   },
   "outputs": [],
   "source": [
    "def kmeans_multi(data, numClusters, nb_iter=10):\n",
    "    centroids = initCentroids(data, numClusters)\n",
    "    iterations = 0\n",
    "    while iterations != nb_iter:\n",
    "        iterations += 1\n",
    "        dataMinDistance = assign_clusters_multi(data, centroids)\n",
    "        newCentroids = compute_centroids_multi(dataMinDistance)\n",
    "        intraClusterDistances = squared_distances_multi(dataMinDistance)\n",
    "\n",
    "        centroids = sc.parallelize(newCentroids.collect())\n",
    "    clusters = assign_clusters(data, centroids)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\tAdapter l’implémentation précédente pour implémenter Spherical k-means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "def cosine_distances_multi(point,listMean):\n",
    "    result = [[0 for j in range(len(point))] for i in range(len(listMean))]\n",
    "    for i, mean in enumerate(listMean):\n",
    "        for j, coordPoint in enumerate(point):\n",
    "            result[i] = distance.cosine(mean[j], coordPoint)\n",
    "    return result \n",
    "def skmeans_assign_clusters_multi(points,centroids):\n",
    "    clusters = centroids.map(lambda a : a[0]).collect()\n",
    "    means = centroids.map(lambda a : a[1]).collect() #Quelle supposition a-t-on faite pour effectuer cette opération ? --> Que les valeurs sont ordonnées (cluster 0 puis cluster 1...)\n",
    "    distances = [squared_distances(point, means) for point in points.collect()]\n",
    "    return sc.parallelize([np.argmax(distance) for distance in distances])\n",
    "\n",
    "def skmeans_multi(data, numClusters, nb_iter=10):\n",
    "    centroids = initCentroids(data, numClusters)\n",
    "    iterations = 0\n",
    "    while iterations != nb_iter:\n",
    "        iterations += 1\n",
    "        dataMinDistance = skmeans_assign_clusters_multi(data, centroids)\n",
    "        newCentroids = compute_centroids_multi(dataMinDistance)\n",
    "        intraClusterDistances = cosine_distances_multi(dataMinDistance)\n",
    "\n",
    "        centroids = sc.parallelize(newCentroids.collect())\n",
    "    clusters = assign_clusters(data, centroids)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Analyser les résultats et comparer à l’implémentation de Spherical k-means du package Coclust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxHCc-3kUUlR",
    "outputId": "908fb6c5-4678-4e43-ee8f-a5fbe4b41ece"
   },
   "outputs": [],
   "source": [
    "skmeans_implementation_labels = skmeans_multi(X,numClusters=2)\n",
    "y_pred_skmeans = skmeans_implementation_labels.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.41      0.45   1401163\n",
      "           1       0.50      0.59      0.54   1398837\n",
      "\n",
      "    accuracy                           0.50   2800000\n",
      "   macro avg       0.50      0.50      0.50   2800000\n",
      "weighted avg       0.50      0.50      0.50   2800000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, skmeans_implementation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résulats obtenus avec le spherical K-means sont plus équilibrés au niveau des metriques de recall et de précision. Cela a pour effet directe d'augmenter le f1-score. \n",
    "Ces résultats sont toutefois peu satisfaisants au regard des exigences généralement attendues dans le  pour ce type de traitement de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0664663067611274e-08"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_mutual_info_score(y_train, skmeans_implementation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La NMI est quant à elle inférieure à celle obtenue en utilisant un kmeans classique. Ce qui suggérait un partionnement encore plus médiore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparaison avec l’implémentation de Spherical k-means du package Coclust "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coclust.clustering.spherical_kmeans import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "coclust_skmeans = SphericalKmeans(n_clusters=2, init=None, max_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " == New init == \n",
      "iteration: 0\n",
      "2502695.2083473476\n",
      "iteration: 1\n",
      "2589355.9742346746\n",
      "iteration: 2\n",
      "2590001.15265664\n",
      "iteration: 3\n",
      "2590101.1862632902\n",
      "iteration: 4\n",
      "2590139.303510783\n",
      "iteration: 5\n",
      "2590145.6502014874\n",
      "iteration: 6\n",
      "2590146.5651609236\n",
      "iteration: 7\n",
      "2590146.8472422464\n",
      "iteration: 8\n",
      "2590147.3171468326\n",
      "iteration: 9\n",
      "2590148.0700637097\n",
      "iteration: 10\n"
     ]
    }
   ],
   "source": [
    "coclust_skmeans.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Classifcation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.41      0.45   1401163\n",
      "           1       0.50      0.59      0.54   1398837\n",
      "\n",
      "    accuracy                           0.50   2800000\n",
      "   macro avg       0.50      0.50      0.50   2800000\n",
      "weighted avg       0.50      0.50      0.50   2800000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, coclust_skmeans.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0664663067611274e-08"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_mutual_info_score(y_train, coclust_skmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les perfomances obtenues sont très similaires, ce qui indique que notre travail d'implémentation de skmeans est correspond bien la formulation de l'algorithme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-  Autres Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous expérimentrons différentes approches de classification dans le but d'améliorer les performances obtenues précédemment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import GaussianMixture\n",
    "gmm = GaussianMixture().setK(2).setSeed(442)\n",
    "model_gmm = gmm.fit(X)\n",
    "predictions_gmm = model_gmm.transform(X_test).withColumn(\"prediction\",F.col(\"prediction\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3328925536524998"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions_gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bisecting k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "\n",
    "bkm = BisectingKMeans().setK(2).setSeed(1)\n",
    "model_bkm = bkm.fit(X)\n",
    "\n",
    "# Make predictions\n",
    "predictions_bkm = model_bkm.transform(X_test).withColumn(\"prediction\",F.col(\"prediction\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3396880201332832"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions_bkm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification supervisée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model_rf = rf.fit(X)\n",
    "\n",
    "# Make predictions.\n",
    "predictions_rf = model_rf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4940638977620505"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 4 (features), two intermediate of size 5 and 4\n",
    "# and output of size 3 (classes)\n",
    "layers = [10, 5, 4, 2]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "# train the model\n",
    "model_mlp = trainer.fit(X)\n",
    "\n",
    "# compute accuracy on the test set\n",
    "result = model_mlp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4940638977620505"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model_gbt = gbt.fit(X)\n",
    "\n",
    "# Make predictions.\n",
    "predictions_gbt = model_gbt.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49182917809562376"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bilan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Les résulats obtenus par les différentes approches de classifications (supervisée ou non) sont relativements similaires.\n",
    "\n",
    "* Aucune approche ne se démarque clairement pour classifier ce jeu de données.\n",
    "* Un classifieur aléatoire sous l'hypothèse que les données sont i.i.d pourrait obtenir des performances similaires."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ProjetBigData.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
